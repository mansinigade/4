{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d68181ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7288/2422133824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# #first install torch in independent cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# import torch.nn as nn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[1;32mraise\u001b[0m  \u001b[1;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Base'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "# #first install torch in independent cell\n",
    "\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Linear transformations\n",
    "        Q = self.query(query)\n",
    "        K = self.key(key)\n",
    "        V = self.value(value)\n",
    "\n",
    "        # Split into heads\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float('-1e20'))\n",
    "\n",
    "        attention = torch.nn.functional.softmax(energy, dim=-1)\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # Reshape and concatenate\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Final linear layer\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, d_model)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Self-attention\n",
    "        attention = self.self_attention(x, x, x, mask)\n",
    "        x = x + self.dropout(attention)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        # Feedforward\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, n_layers, max_seq_length, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, n_heads, ff_dim, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.position_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        positions = torch.arange(0, x.size(1)).expand(x.size(0), x.size(1)).to(self.device)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "ff_dim = 2048\n",
    "n_layers = 6\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "# Create transformer encoder\n",
    "transformer_encoder = TransformerEncoder(d_model, n_heads, ff_dim, n_layers, max_seq_length, dropout)\n",
    "\n",
    "# Dummy input\n",
    "input_data = torch.rand((16, 100, d_model))\n",
    "\n",
    "# Mask for padding\n",
    "padding_mask = (input_data.sum(dim=-1) != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# Forward pass\n",
    "output_data = transformer_encoder(input_data, padding_mask)\n",
    "print(output_data)\n",
    "print(\"Output shape:\", output_data.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e0122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\abhis\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from torch) (1.9)\n",
      "Requirement already satisfied: fsspec in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from torch) (2021.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f4631c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7288/4265195184.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[1;32mraise\u001b[0m  \u001b[1;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Base'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b5ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
